{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data Pipeline\n",
    "This is a Python code sample that implements a piecewise linear regression based on a temperature response function under the context of energy consumption. The original intention of this Jupyter notebook is to: (1) facilitate the work of other team members who are working on different but similar datasets; (2) provide documentation for reproducibility.\n",
    "\n",
    "More information on the research project can be found in this blog post titled [CLIR Postdoctoral Fellowship in Energy Social Science Data Curation Series Part III: Processing a Large Dataset of Electricity Consumption to Study Energy Poverty](https://www.library.cmu.edu/about/news/2022-05/energy-social-science-data-curation-series-part-3)\n",
    "\n",
    "lulinghuang@cmu.edu | 12/21/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import eeweather\n",
    "import eemeter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pytz\n",
    "import itertools\n",
    "import pickle\n",
    "from haversine import haversine\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "import statsmodels.api as sm\n",
    "import censusgeocode as cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: To split a large dataset into chunks by households/customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0\n",
    "Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Delaware Sample.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1\n",
    "Create a python list that contains all unique household/customer ids (no duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the following object name of the python list\n",
    "# intsc_acctsl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsc_acctsl = df.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intsc_acctsl = list(intsc_acctsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50000000104, 50000000203, 50000000401, 50000000419, 50000000435]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intsc_acctsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the households by 2-sized chunks\n",
    "# change 2 to a larger number (e.g., 10,000) for your complete data set\n",
    "intsc_acctsl_chunks = [intsc_acctsl[i:i + 2] for i in range(0, len(intsc_acctsl), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[50000000104, 50000000203], [50000000401, 50000000419], [50000000435]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intsc_acctsl_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2\n",
    "* Turn data from a flat tabular format into a nested format for faster querying\n",
    "* Save data by chunks in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Chunk 1\n",
      "Processing Chunk 2\n",
      "Processing Chunk 3\n"
     ]
    }
   ],
   "source": [
    "for index, chunk in enumerate(intsc_acctsl_chunks):\n",
    "    print(\"Processing Chunk\", index+1)\n",
    "    chunk_ids_s = set(chunk)\n",
    "    chunk_dict = {}\n",
    "\n",
    "    for x in chunk:\n",
    "\n",
    "        info_dict = {}\n",
    "\n",
    "        accttype_dict = {}            \n",
    "        accttype_dict['accttype'] = df.loc[df.index == int(x), 'ACCOUNT_TYPE'].iloc[0]\n",
    "        \n",
    "        address = df.loc[df.index == int(x), 'SERVICE_ADDRESS'].iloc[0]\n",
    "        address_dict = {}\n",
    "        address_dict['address'] = address\n",
    "        #print(address)\n",
    "        #try: \n",
    "        geo = cg.onelineaddress(address)\n",
    "        \n",
    "        if len(geo) != 0:\n",
    "    #print(geo)\n",
    "\n",
    "            geoid = geo[0]['geographies']['2020 Census Blocks'][0]['GEOID']\n",
    "            geoid_dict = {}\n",
    "            geoid_dict['geoid'] = geoid\n",
    "\n",
    "            centlat = geo[0]['geographies']['2020 Census Blocks'][0]['CENTLAT']\n",
    "            centlat_dict = {}\n",
    "            centlat_dict['centlat'] = centlat\n",
    "\n",
    "            centlon = geo[0]['geographies']['2020 Census Blocks'][0]['CENTLON']\n",
    "            centlon_dict = {}\n",
    "            centlon_dict['centlon'] = centlon\n",
    "\n",
    "            tract = geo[0]['geographies']['2020 Census Blocks'][0]['TRACT']\n",
    "            tract_dict = {}\n",
    "            tract_dict['tract'] = tract\n",
    "\n",
    "            state = geo[0]['geographies']['2020 Census Blocks'][0]['STATE']\n",
    "            state_dict = {}\n",
    "            state_dict['state'] = state\n",
    "\n",
    "            location_dict = {}\n",
    "            location_dict['location'] = centlat + ', ' + centlon\n",
    "        #except Exception as e:\n",
    "            #pass\n",
    "        else:\n",
    "            geoid_dict = {}\n",
    "            zcta = address[-5:]\n",
    "            try:\n",
    "                lat, long = eeweather.zcta_to_lat_long(int(zcta))\n",
    "                centlat_dict = {}\n",
    "                centlat_dict['centlat'] = str(lat)\n",
    "                centlon_dict = {}\n",
    "                centlon_dict['centlon'] = str(long)\n",
    "                location_dict = {}\n",
    "                location_dict['location'] = str(lat) + ', ' + str(long)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            tract_dict = {}\n",
    "            state_dict = {}\n",
    "\n",
    "        date_dict = {}\n",
    "        date_dict['date'] = df.loc[df.index == int(x), 'READ_DATE'].values\n",
    "\n",
    "        elec_dict = {}\n",
    "        elec_dict['daily_elec'] = df.loc[df.index == int(x), 'READ_VALUE'].values\n",
    "\n",
    "        info_dict[x] = accttype_dict\n",
    "        info_dict[x].update(address_dict)\n",
    "        #try:\n",
    "        info_dict[x].update(geoid_dict)\n",
    "        info_dict[x].update(centlat_dict)\n",
    "        info_dict[x].update(centlon_dict)\n",
    "        info_dict[x].update(tract_dict)\n",
    "        info_dict[x].update(state_dict)\n",
    "        info_dict[x].update(location_dict)\n",
    "        #except Exception as e:\n",
    "            #pass\n",
    "        info_dict[x].update(date_dict)\n",
    "        info_dict[x].update(elec_dict)\n",
    "        chunk_dict.update(info_dict)\n",
    "    \n",
    "    # For variables that have multiple values for each household\n",
    "    # Need to turn these data into a list first\n",
    "    for k, v in chunk_dict.items():\n",
    "        for key, val in v.items():\n",
    "            if key.endswith('date'):\n",
    "                v[key] = val.tolist()\n",
    "            elif key.endswith('elec'):\n",
    "                v[key] = val.tolist()\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    # Write to file\n",
    "    # assuming there is folder called 'data_chunks' created in the current working directory\n",
    "    with open('data_chunks_geo\\\\data_chunk'+str(index+1).zfill(2)+'.json', 'w') as fp:\n",
    "        json.dump(chunk_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the saved data chunks json files can be reused in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get daily temperature data for all chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0\n",
    "Create a set of unique locations (unique coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_nohidden(path):\n",
    "    for f in os.listdir(path):\n",
    "        if not f.startswith('.'):\n",
    "            yield f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunks_dir = os.path.join(cwd, \"data_chunks_geo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = list(listdir_nohidden(data_chunks_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_chunk01.json', 'data_chunk02.json', 'data_chunk03.json']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: data_chunk01.json\n",
      "Processing: data_chunk02.json\n",
      "Processing: data_chunk03.json\n"
     ]
    }
   ],
   "source": [
    "chunks_location_s = set()\n",
    "for chunk in chunks:\n",
    "    print(\"Processing:\", chunk)\n",
    "    chunk_location_s = set()\n",
    "    with open(os.path.join(data_chunks_dir, chunk), 'r') as filehandle:\n",
    "        chunk_data = json.load(filehandle)\n",
    "    for k, v in chunk_data.items():\n",
    "        chunk_location_s.add(v['location'])\n",
    "    chunks_location_s = chunks_location_s.union(chunk_location_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+39.6267430, -075.6913967',\n",
       " '+39.7283333, -075.5743227',\n",
       " '+39.7637300, -075.4925055',\n",
       " '+39.7643338, -075.4909829',\n",
       " '39.7142863834481, -75.7396582466419'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_location_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "Get the closest station (that has enough temperature data) for each coordinate and append temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(2017, 8, 1, tzinfo=pytz.UTC)\n",
    "end_date = datetime.datetime(2018, 1, 1, tzinfo=pytz.UTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_location_station = {}\n",
    "for x in chunks_location_s:\n",
    "    \n",
    "    lat = x.split(', ')[0]\n",
    "    long = x.split(', ')[1]\n",
    "\n",
    "    '''\n",
    "    Go through each ranked_stations (from the closest to more distant stations).\n",
    "    Retrieve weather data until found.\n",
    "    '''\n",
    "    ranked_stations = eeweather.rank_stations(float(lat), float(long))\n",
    "    for y in range(1, 101):\n",
    "        station, warnings = eeweather.select_station(ranked_stations, rank = y)\n",
    "        try:\n",
    "\n",
    "            tempC = station.load_isd_hourly_temp_data(start_date, end_date)\n",
    "            tempC_daily_avg = tempC[0].resample('D').mean()\n",
    "            n_nan_tempC_days = tempC_daily_avg.isna().sum()\n",
    "            str_error = None\n",
    "\n",
    "        except Exception as e:\n",
    "            str_error = e\n",
    "            pass\n",
    "\n",
    "        if str_error or n_nan_tempC_days > 23: # Out of 153 days, maybe we need at least 130 days' temp data\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    tempF = tempC[0] * 1.8 + 32\n",
    "\n",
    "    chunks_location_station[x] = {'station': station}\n",
    "    chunks_location_station[x].update({'tempF': tempF})\n",
    "    chunks_location_station[x].update({'usaf_id': station.usaf_id})\n",
    "    chunks_location_station[x].update({'wban_ids': station.wban_ids})\n",
    "    chunks_location_station[x].update({'station_distance': ranked_stations[ranked_stations['rank'] == y]['distance_meters'].values.tolist()[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+39.7283333, -075.5743227': {'station': ISDStation('724180'),\n",
       "  'tempF': 2017-08-01 00:00:00+00:00    80.08736\n",
       "  2017-08-01 01:00:00+00:00    77.69876\n",
       "  2017-08-01 02:00:00+00:00    75.06716\n",
       "  2017-08-01 03:00:00+00:00    73.93100\n",
       "  2017-08-01 04:00:00+00:00    73.35176\n",
       "                                 ...   \n",
       "  2017-12-31 20:00:00+00:00    15.94940\n",
       "  2017-12-31 21:00:00+00:00    14.02736\n",
       "  2017-12-31 22:00:00+00:00    11.66036\n",
       "  2017-12-31 23:00:00+00:00    10.42250\n",
       "  2018-01-01 00:00:00+00:00     9.03200\n",
       "  Freq: H, Length: 3673, dtype: float64,\n",
       "  'usaf_id': '724180',\n",
       "  'wban_ids': ['13781'],\n",
       "  'station_distance': 6616.106508257432},\n",
       " '+39.7643338, -075.4909829': {'station': ISDStation('998265'),\n",
       "  'tempF': 2017-08-01 00:00:00+00:00    83.85206\n",
       "  2017-08-01 01:00:00+00:00    81.52700\n",
       "  2017-08-01 02:00:00+00:00    79.08350\n",
       "  2017-08-01 03:00:00+00:00    77.46350\n",
       "  2017-08-01 04:00:00+00:00    76.64000\n",
       "                                 ...   \n",
       "  2017-12-31 20:00:00+00:00    15.44900\n",
       "  2017-12-31 21:00:00+00:00    14.54594\n",
       "  2017-12-31 22:00:00+00:00    13.82594\n",
       "  2017-12-31 23:00:00+00:00    13.46000\n",
       "  2018-01-01 00:00:00+00:00    12.65450\n",
       "  Freq: H, Length: 3673, dtype: float64,\n",
       "  'usaf_id': '998265',\n",
       "  'wban_ids': ['99999'],\n",
       "  'station_distance': 8622.68473551755},\n",
       " '+39.7637300, -075.4925055': {'station': ISDStation('998265'),\n",
       "  'tempF': 2017-08-01 00:00:00+00:00    83.85206\n",
       "  2017-08-01 01:00:00+00:00    81.52700\n",
       "  2017-08-01 02:00:00+00:00    79.08350\n",
       "  2017-08-01 03:00:00+00:00    77.46350\n",
       "  2017-08-01 04:00:00+00:00    76.64000\n",
       "                                 ...   \n",
       "  2017-12-31 20:00:00+00:00    15.44900\n",
       "  2017-12-31 21:00:00+00:00    14.54594\n",
       "  2017-12-31 22:00:00+00:00    13.82594\n",
       "  2017-12-31 23:00:00+00:00    13.46000\n",
       "  2018-01-01 00:00:00+00:00    12.65450\n",
       "  Freq: H, Length: 3673, dtype: float64,\n",
       "  'usaf_id': '998265',\n",
       "  'wban_ids': ['99999'],\n",
       "  'station_distance': 8764.102152564059},\n",
       " '39.7142863834481, -75.7396582466419': {'station': ISDStation('724180'),\n",
       "  'tempF': 2017-08-01 00:00:00+00:00    80.08736\n",
       "  2017-08-01 01:00:00+00:00    77.69876\n",
       "  2017-08-01 02:00:00+00:00    75.06716\n",
       "  2017-08-01 03:00:00+00:00    73.93100\n",
       "  2017-08-01 04:00:00+00:00    73.35176\n",
       "                                 ...   \n",
       "  2017-12-31 20:00:00+00:00    15.94940\n",
       "  2017-12-31 21:00:00+00:00    14.02736\n",
       "  2017-12-31 22:00:00+00:00    11.66036\n",
       "  2017-12-31 23:00:00+00:00    10.42250\n",
       "  2018-01-01 00:00:00+00:00     9.03200\n",
       "  Freq: H, Length: 3673, dtype: float64,\n",
       "  'usaf_id': '724180',\n",
       "  'wban_ids': ['13781'],\n",
       "  'station_distance': 12306.039409108102},\n",
       " '+39.6267430, -075.6913967': {'station': ISDStation('724180'),\n",
       "  'tempF': 2017-08-01 00:00:00+00:00    80.08736\n",
       "  2017-08-01 01:00:00+00:00    77.69876\n",
       "  2017-08-01 02:00:00+00:00    75.06716\n",
       "  2017-08-01 03:00:00+00:00    73.93100\n",
       "  2017-08-01 04:00:00+00:00    73.35176\n",
       "                                 ...   \n",
       "  2017-12-31 20:00:00+00:00    15.94940\n",
       "  2017-12-31 21:00:00+00:00    14.02736\n",
       "  2017-12-31 22:00:00+00:00    11.66036\n",
       "  2017-12-31 23:00:00+00:00    10.42250\n",
       "  2018-01-01 00:00:00+00:00     9.03200\n",
       "  Freq: H, Length: 3673, dtype: float64,\n",
       "  'usaf_id': '724180',\n",
       "  'wban_ids': ['13781'],\n",
       "  'station_distance': 9013.857279123009}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_location_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point 1\n",
    "# The above procedure may take a long time to run if data is large.\n",
    "# Save the above dictionary to file to save time whenever you reopen a notebook\n",
    "with open(os.path.join(cwd,'intermediate_data','chunks_location_station.pickle'), 'wb') as fp:\n",
    "    pickle.dump(chunks_location_station, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Retrieve meteorological data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0\n",
    "Get meteorological data from NOAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation on how the met data are retrieved from NOAA\n",
    "### 1. Problem: eeweather only returns temperature data\n",
    "Therefore, we must go to NOAA's data website (https://www.ncdc.noaa.gov/cdo-web/datasets).\n",
    "### 2. Choose and download datasets\n",
    "In the above website, choose Local Climatological Data. Click 'search tool.' Select 'State -> Delaware.' Then, 3 stations are shown.\n",
    "### 3. Continue on Point 2\n",
    "Click 'ADD TO CART' for download later. Download all 3 stations. When done, go checkout the selected stations. Choose LCD CSV as the output format, and select date range. And just follow the instructions for download.\n",
    "### 4. Take a look at the data\n",
    "In OpenRefine (or similar tool), we'll use two categories in the 'REPORT_TYPE' column: SOD (this is daily summary data) and FM-15 (this is hourly data). We first choose SOD rows only, and check which stations have the complete data for all days. We will only use those stations that have the full data. Refer the column name meaning to the LCD documentation (found on the link in Point 1, above). For SOD, average daily precipitation and wind speed are available. Mysteriously, we have to calculate the daily average for air pressure at sea level and relative humidity based on the hourly data (which is done in this section). Also note that the STATION identifier is: usaf_id + wban_id (no space). For this specific time span, only two stations have complete data (wban id: 13764 & 13781)\n",
    "### 5. One way to find a weather station's coordinate\n",
    "We need the station coordinates so that we can find the closest station to each household coordinate. Go to link in Point 1, choose Local Climatological Data. Click 'mapping tool.' Zoom in to Delaware. There are 3 stations in Delaware represented in red dots. The one in the north and the one in the south are the two stations that have the complete data. On the left, click the 'tool' icon and click 'Identity.' On the map, click one of the stations to see the station details (coordinates included)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "Prepare the downloaded meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load downloaded daily data\n",
    "# You need to redownload the data if you change the time span\n",
    "df_met = pd.read_csv(os.path.join(cwd, \"met_170801_171231_daily.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two stations * 153 days = 306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met['DATE'] = pd.to_datetime(df_met['DATE'], format=\"%Y-%m-%d\").dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met['DATE'] = pd.to_datetime(df_met['DATE'], format=\"%Y-%m-%d\").dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load downloaded hourly data\n",
    "# You need to redownload the data if you change the time span\n",
    "df_met_hourly = pd.read_csv(os.path.join(cwd, \"met_170801_171231_hourly.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7344"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_met_hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two stations * 153 days * 24 hours = 7,344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met_hourly['DATE'] = pd.to_datetime(df_met_hourly['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_met_hourly.groupby('STATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the daily average based on hourly data\n",
    "df_met_hourly_dailyavg = pd.DataFrame()\n",
    "for name, station in grouped:\n",
    "    davg_station = station.groupby(pd.Grouper(freq='D', key='DATE')).mean()\n",
    "    df_met_hourly_dailyavg = df_met_hourly_dailyavg.append(davg_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>HourlyRelativeHumidity</th>\n",
       "      <th>HourlySeaLevelPressure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-01</th>\n",
       "      <td>72409313764</td>\n",
       "      <td>67.375000</td>\n",
       "      <td>30.052917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-02</th>\n",
       "      <td>72409313764</td>\n",
       "      <td>69.666667</td>\n",
       "      <td>30.069583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-03</th>\n",
       "      <td>72409313764</td>\n",
       "      <td>79.125000</td>\n",
       "      <td>30.092917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-04</th>\n",
       "      <td>72409313764</td>\n",
       "      <td>75.375000</td>\n",
       "      <td>29.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-05</th>\n",
       "      <td>72409313764</td>\n",
       "      <td>67.708333</td>\n",
       "      <td>29.944167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                STATION  HourlyRelativeHumidity  HourlySeaLevelPressure\n",
       "DATE                                                                   \n",
       "2017-08-01  72409313764               67.375000               30.052917\n",
       "2017-08-02  72409313764               69.666667               30.069583\n",
       "2017-08-03  72409313764               79.125000               30.092917\n",
       "2017-08-04  72409313764               75.375000               29.995000\n",
       "2017-08-05  72409313764               67.708333               29.944167"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_met_hourly_dailyavg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_met_hourly_dailyavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes\n",
    "df_met_all = pd.merge(df_met, df_met_hourly_dailyavg,  how='left', on = ['DATE','STATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_met_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>REPORT_TYPE</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>AWND</th>\n",
       "      <th>BackupDirection</th>\n",
       "      <th>BackupDistance</th>\n",
       "      <th>BackupDistanceUnit</th>\n",
       "      <th>BackupElements</th>\n",
       "      <th>BackupElevation</th>\n",
       "      <th>...</th>\n",
       "      <th>ShortDurationPrecipitationValue080</th>\n",
       "      <th>ShortDurationPrecipitationValue100</th>\n",
       "      <th>ShortDurationPrecipitationValue120</th>\n",
       "      <th>ShortDurationPrecipitationValue150</th>\n",
       "      <th>ShortDurationPrecipitationValue180</th>\n",
       "      <th>Sunrise</th>\n",
       "      <th>Sunset</th>\n",
       "      <th>WindEquipmentChangeDate</th>\n",
       "      <th>HourlyRelativeHumidity_y</th>\n",
       "      <th>HourlySeaLevelPressure_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72409313764</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>SOD</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>503</td>\n",
       "      <td>1912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.375</td>\n",
       "      <td>30.052917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATION       DATE REPORT_TYPE  SOURCE  AWND  BackupDirection  \\\n",
       "0  72409313764 2017-08-01       SOD         6   NaN              NaN   \n",
       "\n",
       "   BackupDistance  BackupDistanceUnit  BackupElements  BackupElevation  ...  \\\n",
       "0             NaN                 NaN             NaN              NaN  ...   \n",
       "\n",
       "   ShortDurationPrecipitationValue080  ShortDurationPrecipitationValue100  \\\n",
       "0                                 NaN                                 NaN   \n",
       "\n",
       "   ShortDurationPrecipitationValue120  ShortDurationPrecipitationValue150  \\\n",
       "0                                 NaN                                 NaN   \n",
       "\n",
       "   ShortDurationPrecipitationValue180  Sunrise  Sunset  \\\n",
       "0                                 NaN      503    1912   \n",
       "\n",
       "   WindEquipmentChangeDate  HourlyRelativeHumidity_y  HourlySeaLevelPressure_y  \n",
       "0                      NaN                    67.375                 30.052917  \n",
       "\n",
       "[1 rows x 126 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_met_all.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need all the columns, so:\n",
    "cols = ['STATION', 'DATE', 'DailyAverageWindSpeed', 'DailyPrecipitation', 'HourlyRelativeHumidity_y', 'HourlySeaLevelPressure_y']\n",
    "df_met_usable = df_met_all.loc[:,cols] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DailyAverageWindSpeed</th>\n",
       "      <th>DailyPrecipitation</th>\n",
       "      <th>HourlyRelativeHumidity_y</th>\n",
       "      <th>HourlySeaLevelPressure_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72409313764</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>67.375000</td>\n",
       "      <td>30.052917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72409313764</td>\n",
       "      <td>2017-08-02</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>69.666667</td>\n",
       "      <td>30.069583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72409313764</td>\n",
       "      <td>2017-08-03</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.81</td>\n",
       "      <td>79.125000</td>\n",
       "      <td>30.092917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72409313764</td>\n",
       "      <td>2017-08-04</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>75.375000</td>\n",
       "      <td>29.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72409313764</td>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>67.708333</td>\n",
       "      <td>29.944167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>72418013781</td>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>30.521667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>72418013781</td>\n",
       "      <td>2017-12-28</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>30.575417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>72418013781</td>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.208333</td>\n",
       "      <td>30.344167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>72418013781</td>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.02</td>\n",
       "      <td>74.166667</td>\n",
       "      <td>30.097083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>72418013781</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.750000</td>\n",
       "      <td>30.322083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         STATION       DATE  DailyAverageWindSpeed DailyPrecipitation  \\\n",
       "0    72409313764 2017-08-01                    3.9               0.00   \n",
       "1    72409313764 2017-08-02                    5.8               0.00   \n",
       "2    72409313764 2017-08-03                    5.3               1.81   \n",
       "3    72409313764 2017-08-04                    7.5               0.00   \n",
       "4    72409313764 2017-08-05                    6.3               0.00   \n",
       "..           ...        ...                    ...                ...   \n",
       "301  72418013781 2017-12-27                   10.3               0.00   \n",
       "302  72418013781 2017-12-28                   11.0               0.00   \n",
       "303  72418013781 2017-12-29                    5.6               0.00   \n",
       "304  72418013781 2017-12-30                    6.8               0.02   \n",
       "305  72418013781 2017-12-31                   11.8               0.00   \n",
       "\n",
       "     HourlyRelativeHumidity_y  HourlySeaLevelPressure_y  \n",
       "0                   67.375000                 30.052917  \n",
       "1                   69.666667                 30.069583  \n",
       "2                   79.125000                 30.092917  \n",
       "3                   75.375000                 29.995000  \n",
       "4                   67.708333                 29.944167  \n",
       "..                        ...                       ...  \n",
       "301                 44.250000                 30.521667  \n",
       "302                 46.666667                 30.575417  \n",
       "303                 52.208333                 30.344167  \n",
       "304                 74.166667                 30.097083  \n",
       "305                 53.750000                 30.322083  \n",
       "\n",
       "[306 rows x 6 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_met_usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df_met_usable.rename({'DailyAverageWindSpeed': 'ws', \n",
    "                      'DailyPrecipitation': 'prcp',\n",
    "                      'HourlyRelativeHumidity_y': 'hum',\n",
    "                      'HourlySeaLevelPressure_y': 'prss'}, \n",
    "                     axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up some strings in the prcp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met_usable[\"prcp\"] = df_met_usable[\"prcp\"].replace({'T':'0.0', 'Ts':'0.0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met_usable[\"prcp\"] = df_met_usable[\"prcp\"].str.rstrip('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_met_usable[\"prcp\"] = pd.to_numeric(df_met_usable[\"prcp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point 2\n",
    "# Save the above df to file to save time whenever you reopen a notebook\n",
    "df_met_usable.to_csv(os.path.join(cwd, 'intermediate_data', 'df_met_usable.csv'), encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "Determine the closest met station to a given coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72409313764, 72418013781], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_met_usable.STATION.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, the last five digits represent the wban id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually link coordinates to the two stations\n",
    "# See Point 5 in the documentation for NOAA data, above\n",
    "station_coord = {\"72418013781\": (39.67444, -75.60566),\n",
    "                \"72409313764\": (38.68974, -75.36246)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each coordinate to its closest met station\n",
    "chunks_coord_metstation = {}\n",
    "for x in chunks_location_s:\n",
    "    lat = x.split(', ')[0]\n",
    "    long = x.split(', ')[1]\n",
    "\n",
    "    coord = (float(lat), float(long))\n",
    "    eval_dist = {}\n",
    "    for k, v in station_coord.items():\n",
    "        st_coord = v\n",
    "        eval_dist.update({k: haversine(coord, st_coord)})\n",
    "    closest = min(eval_dist, key=eval_dist.get)\n",
    "\n",
    "    chunks_coord_metstation.update({x: closest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'39.76324457383512, -75.49142431540835': '72418013781',\n",
       " '39.72922397692623, -75.57507134793907': '72418013781',\n",
       " '39.62778147946794, -75.69264687308187': '72418013781',\n",
       " '39.7644823205844, -75.4916653037617': '72418013781',\n",
       " '39.717691494890744, -75.61040457609896': '72418013781'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_coord_metstation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point 3\n",
    "# Save the above dictionary to file to save time whenever you reopen a notebook\n",
    "with open(os.path.join(cwd,'intermediate_data','chunks_coord_metstation.pickle'), 'wb') as fp:\n",
    "    pickle.dump(chunks_coord_metstation, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0\n",
    "Define a group of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_rows(design_matrix):\n",
    "    rows_tofix = design_matrix.loc[design_matrix['n_days_kept'] > 1.0]\n",
    "    rows_tofix = rows_tofix.copy()\n",
    "    if len(rows_tofix) > 0:\n",
    "        for index, row in rows_tofix.iterrows():\n",
    "            # Fix meter value\n",
    "            n_days = row['n_days_kept']\n",
    "            rows_tofix.loc[index, 'meter_value'] = row['meter_value'] * n_days\n",
    "\n",
    "            # Fix daily average temp\n",
    "            correct_tempFs = tempF.loc[pd.date_range(start=index.date(), periods=24, freq='H').tz_localize('UTC')]\n",
    "            correct_tempFs_mean = correct_tempFs.values.mean()\n",
    "            rows_tofix.loc[index, 'temperature_mean'] = correct_tempFs_mean\n",
    "\n",
    "            # Fix CDDs and HDDs\n",
    "            cdd_cols = ['cdd_' + str(x) for x in range(30,91)]\n",
    "            hdd_cols = ['hdd_' + str(x) for x in range(30,91)]\n",
    "\n",
    "            new_cdds = []\n",
    "            new_hdds = []\n",
    "            for x in range(30,91):\n",
    "                # Refer to: https://docs.caltrack.org/en/latest/methods.html\n",
    "                new_cdds.append(max(correct_tempFs_mean - x, 0))\n",
    "                new_hdds.append(max(x - correct_tempFs_mean, 0))\n",
    "\n",
    "            rows_tofix.loc[index, cdd_cols] = new_cdds\n",
    "            rows_tofix.loc[index, hdd_cols] = new_hdds\n",
    "\n",
    "        # Replace old problematic rows with fixed rows\n",
    "        design_matrix.loc[rows_tofix.index, :] = rows_tofix[:]\n",
    "        return design_matrix\n",
    "    else:\n",
    "        return design_matrix\n",
    "\n",
    "\n",
    "def create_day_vars(eemeter_designm, start_date, end_date):\n",
    "    '''\n",
    "    Take in a design_matrix created by eemeter, add the following dummy variables (0 = No OR 1 = Yes) regarding a specific day:\n",
    "    Weekend\n",
    "    Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\n",
    "    January, February, March, April, May, June, July, August, September, October, November, December\n",
    "    Holiday\n",
    "    Return a new design_matrix with the above additional variables\n",
    "    '''\n",
    "    # Day of week\n",
    "    eemeter_designm['dow'] = eemeter_designm.index.dayofweek # Monday=0, Sunday=6\n",
    "    \n",
    "    # Weekend dummy\n",
    "    eemeter_designm['dme_wnd'] = np.where(eemeter_designm['dow'] >= 5, 1, 0) # Monday=0, Sunday=6\n",
    "    \n",
    "    # Day of week dummy vars\n",
    "    # eemeter_designm['mon'] = np.where(eemeter_designm['dow'] == 0, 1, 0) # Monday=0, Sunday=6\n",
    "    eemeter_designm['dme_tue'] = np.where(eemeter_designm['dow'] == 1, 1, 0)\n",
    "    eemeter_designm['dme_wed'] = np.where(eemeter_designm['dow'] == 2, 1, 0)\n",
    "    eemeter_designm['dme_thu'] = np.where(eemeter_designm['dow'] == 3, 1, 0)\n",
    "    eemeter_designm['dme_fri'] = np.where(eemeter_designm['dow'] == 4, 1, 0)\n",
    "    eemeter_designm['dme_sat'] = np.where(eemeter_designm['dow'] == 5, 1, 0)\n",
    "    eemeter_designm['dme_sun'] = np.where(eemeter_designm['dow'] == 6, 1, 0)\n",
    "    \n",
    "    # Month of year\n",
    "    eemeter_designm['moy'] = eemeter_designm.index.month # month-of-year\n",
    "    \n",
    "    months_in_data = list(eemeter_designm['moy'].unique())\n",
    "    \n",
    "    # Month of year dummy vars\n",
    "    if len(months_in_data) == 12:\n",
    "        for month in months_in_data[1:]:\n",
    "            eemeter_designm['dme_month_' + str(month)] = np.where(eemeter_designm['moy'] == month, 1, 0)\n",
    "    elif eemeter_designm.index[-1].day == 1:\n",
    "        for month in months_in_data[1:-1]:\n",
    "            eemeter_designm['dme_month_' + str(month)] = np.where(eemeter_designm['moy'] == month, 1, 0)\n",
    "    else:\n",
    "        for month in months_in_data[1:]:\n",
    "            eemeter_designm['dme_month_' + str(month)] = np.where(eemeter_designm['moy'] == month, 1, 0)\n",
    "    \n",
    "    # Holiday dummy\n",
    "    dr = pd.date_range(start=start_date, end=end_date)\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "    eemeter_designm['dme_holiday'] = np.where(eemeter_designm.index.isin(holidays), 1, 0)\n",
    "    \n",
    "    return eemeter_designm\n",
    "\n",
    "\n",
    "def check_dd_sufficiency(designm,\n",
    "                           minimum_non_zero_cdd = 10,\n",
    "                           minimum_non_zero_hdd = 10,\n",
    "                           minimum_total_cdd = 20.0,\n",
    "                           minimum_total_hdd = 20.0):\n",
    "    '''\n",
    "    Check whether a cdd column or an hdd column complies\n",
    "    with CalTRACK's default standard:\n",
    "    https://eemeter.readthedocs.io/api.html#eemeter.fit_caltrack_usage_per_day_model\n",
    "    '''\n",
    "    cdd_cols = ['cdd_' + str(x) for x in range(30,91)]\n",
    "    hdd_cols = ['hdd_' + str(x) for x in range(30,91)]\n",
    "    cdd_cols_checked = []\n",
    "    hdd_cols_checked = []\n",
    "    for cdd in cdd_cols:\n",
    "        if (len(designm[designm[cdd] != 0.0]) >= 10 and\n",
    "            designm[cdd].sum() >= 20.0):\n",
    "            cdd_cols_checked.append(cdd)\n",
    "    for hdd in hdd_cols:\n",
    "        if (len(designm[designm[hdd] != 0.0]) >= 10 and\n",
    "            designm[hdd].sum() >= 20.0):\n",
    "            hdd_cols_checked.append(hdd)\n",
    "            \n",
    "    return cdd_cols_checked, hdd_cols_checked\n",
    "\n",
    "\n",
    "def append_met_cols(designm, metm, zipn, zipn_metstation):\n",
    "    metstation_id = zipn_metstation[zipn]\n",
    "    met_data = metm.loc[metm['STATION']==int(metstation_id),:]\n",
    "    \n",
    "    # Set index as datetimeindex (with date only)\n",
    "    # Purpose: Make it easier to append met cols to designm later \n",
    "    met_data.set_index(met_data['DATE'], inplace=True)\n",
    "    met_data.index = pd.to_datetime(met_data.index)\n",
    "\n",
    "    designm['met_ws'] = met_data['ws']\n",
    "    designm['met_prcp'] = met_data['prcp']\n",
    "    designm['met_hum'] = met_data['hum']\n",
    "    designm['met_prss'] = met_data['prss']\n",
    "    \n",
    "    return designm\n",
    "\n",
    "\n",
    "def fit_caltrack_customized(designm,\n",
    "                            fit_cdd_only=False, # y ~ cdd\n",
    "                            fit_hdd_only=False, # y ~ hdd\n",
    "                            fit_cdd_hdd_only=False, # y ~ cdd + hdd\n",
    "                            fit_all=True,\n",
    "                            save_candidate_results=False\n",
    "                            ):\n",
    "    \n",
    "    cdd_cols_checked = check_dd_sufficiency(designm)[0]\n",
    "    hdd_cols_checked = check_dd_sufficiency(designm)[1]\n",
    "    \n",
    "    designm['met_prcp_sq'] = designm['met_prcp'] ** 2\n",
    "    \n",
    "    time_vars = [col for col in designm if col.startswith('dme_')]\n",
    "    met_vars = ['met_prcp_sq', 'met_prcp', 'met_hum', 'met_ws']\n",
    "    \n",
    "    all_vars = time_vars + met_vars + ['const']\n",
    "    \n",
    "    # if excluding meteorological variables:\n",
    "    # all_vars = time_vars + ['const']\n",
    "    \n",
    "    res_intercept_only = sm.OLS(designm['meter_value'], designm[all_vars], missing='drop').fit()\n",
    "    intercept_only_rsqa = res_intercept_only.rsquared_adj\n",
    "    \n",
    "    if fit_cdd_only is True or fit_hdd_only is True or fit_cdd_hdd_only is True:\n",
    "        fit_all = False\n",
    "    \n",
    "    if fit_cdd_only is True:\n",
    "        candidate_cdd_list = []\n",
    "        best_fit = {}\n",
    "        eval_rsqa = {}\n",
    "        counter = 0\n",
    "        for cdd in cdd_cols_checked:    \n",
    "            all_vars_cdd = all_vars + [cdd]\n",
    "            res = sm.OLS(designm['meter_value'], designm[all_vars_cdd], missing='drop').fit()\n",
    "            if save_candidate_results is True:\n",
    "                candidate_cdd_list.append({cdd: res})\n",
    "            if counter == 0:\n",
    "                eval_rsqa[cdd] = res.rsquared_adj\n",
    "                best_fit[cdd] = res\n",
    "                counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                params_dict = res.params.to_dict()\n",
    "                pvalues_dict = res.pvalues.to_dict()\n",
    "                c_slope = params_dict[cdd]\n",
    "                c_slope_p = pvalues_dict[cdd]\n",
    "                intercept = params_dict['const']\n",
    "                if list(eval_rsqa.values())[0] < res.rsquared_adj and c_slope > 0 and c_slope_p < .1 and intercept >= 0:\n",
    "                    eval_rsqa = {}\n",
    "                    eval_rsqa[cdd] = res.rsquared_adj\n",
    "                    best_fit = {}\n",
    "                    best_fit[cdd] = res\n",
    "                    #counter += 1\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        # Compare with the intercept only model\n",
    "        if list(eval_rsqa.values())[0] < intercept_only_rsqa:\n",
    "            best_fit = {}\n",
    "            best_fit['intercept_only'] = res_intercept_only\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if save_candidate_results:\n",
    "            return best_fit, candidate_cdd_list \n",
    "        else:\n",
    "            return best_fit\n",
    "    \n",
    "    elif fit_hdd_only is True:\n",
    "        candidate_hdd_list = []\n",
    "        best_fit = {}\n",
    "        eval_rsqa = {}\n",
    "        counter = 0\n",
    "        for hdd in hdd_cols_checked:    \n",
    "            all_vars_hdd = all_vars + [hdd]\n",
    "            res = sm.OLS(designm['meter_value'], designm[all_vars_hdd], missing='drop').fit()\n",
    "            if save_candidate_results is True:\n",
    "                candidate_hdd_list.append({hdd: res})\n",
    "            if counter == 0:\n",
    "                eval_rsqa[hdd] = res.rsquared_adj\n",
    "                best_fit[hdd] = res\n",
    "                counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                params_dict = res.params.to_dict()\n",
    "                pvalues_dict = res.pvalues.to_dict()\n",
    "                h_slope = params_dict[hdd]\n",
    "                h_slope_p = pvalues_dict[hdd]\n",
    "                intercept = params_dict['const']\n",
    "                if list(eval_rsqa.values())[0] < res.rsquared_adj and h_slope > 0 and h_slope_p < .1 and intercept >= 0:\n",
    "                    eval_rsqa = {}\n",
    "                    eval_rsqa[hdd] = res.rsquared_adj\n",
    "                    best_fit = {}\n",
    "                    best_fit[hdd] = res\n",
    "                    #counter += 1\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "        # Compare with the intercept only model\n",
    "        if list(eval_rsqa.values())[0] < intercept_only_rsqa:\n",
    "            best_fit = {}\n",
    "            best_fit['intercept_only'] = res_intercept_only\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if save_candidate_results:\n",
    "            return best_fit, candidate_hdd_list \n",
    "        else:\n",
    "            return best_fit\n",
    "    \n",
    "    elif fit_cdd_hdd_only is True:\n",
    "        candidate_cdd_hdd_list = []\n",
    "        best_fit = {}\n",
    "        eval_rsqa = {}\n",
    "        counter = 0\n",
    "        \n",
    "        for x in itertools.product(cdd_cols_checked, hdd_cols_checked):\n",
    "            if x[0][-2:] > x[1][-2:]:\n",
    "                cdd = x[0]\n",
    "                hdd = x[1]\n",
    "                all_vars_cdd_hdd = all_vars + [cdd, hdd]\n",
    "                res = sm.OLS(designm['meter_value'], designm[all_vars_cdd_hdd], missing='drop').fit()\n",
    "                if save_candidate_results is True:\n",
    "                    candidate_cdd_hdd_list.append({cdd + \"+\" + hdd: res})\n",
    "                \n",
    "                if counter == 0:\n",
    "                    eval_rsqa[cdd + \"+\" + hdd] = res.rsquared_adj\n",
    "                    best_fit[cdd + \"+\" + hdd] = res\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    params_dict = res.params.to_dict()\n",
    "                    pvalues_dict = res.pvalues.to_dict()\n",
    "                    c_slope = params_dict[cdd]\n",
    "                    c_slope_p = pvalues_dict[cdd]\n",
    "                    h_slope = params_dict[hdd]\n",
    "                    h_slope_p = pvalues_dict[hdd]\n",
    "                    intercept = params_dict['const']\n",
    "                    if list(eval_rsqa.values())[0] < res.rsquared_adj and c_slope > 0 and c_slope_p < .1 and h_slope > 0 and h_slope_p < .1 and intercept >= 0:\n",
    "                        eval_rsqa = {}\n",
    "                        eval_rsqa[cdd + \"+\" + hdd] = res.rsquared_adj\n",
    "                        best_fit = {}\n",
    "                        best_fit[cdd + \"+\" + hdd] = res\n",
    "                        #counter += 1\n",
    "                    else:\n",
    "                        continue\n",
    "        \n",
    "        # Compare with the intercept only model\n",
    "        if list(eval_rsqa.values())[0] < intercept_only_rsqa:\n",
    "            best_fit = {}\n",
    "            best_fit['intercept_only'] = res_intercept_only\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if save_candidate_results:\n",
    "            return best_fit, candidate_cdd_hdd_list \n",
    "        else:\n",
    "            return best_fit\n",
    "    \n",
    "    if fit_all is True:\n",
    "        candidate_all_list = []\n",
    "        best_fit_all = {}\n",
    "        eval_rsqa_all = {}\n",
    "        \n",
    "        \n",
    "        best_fit_cdd = {}\n",
    "        eval_rsqa_cdd = {}\n",
    "        counter = 0\n",
    "        for cdd in cdd_cols_checked:    \n",
    "            all_vars_cdd = all_vars + [cdd]\n",
    "            res = sm.OLS(designm['meter_value'], designm[all_vars_cdd], missing='drop').fit()\n",
    "            if save_candidate_results:\n",
    "                candidate_all_list.append({cdd: res})\n",
    "            \n",
    "            if counter == 0:\n",
    "                eval_rsqa_cdd[cdd] = res.rsquared_adj\n",
    "                best_fit_cdd[cdd] = res\n",
    "                counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                params_dict = res.params.to_dict()\n",
    "                pvalues_dict = res.pvalues.to_dict()\n",
    "                c_slope = params_dict[cdd]\n",
    "                c_slope_p = pvalues_dict[cdd]\n",
    "                intercept = params_dict['const']\n",
    "                if list(eval_rsqa_cdd.values())[0] < res.rsquared_adj and c_slope > 0 and c_slope_p < .1 and intercept >= 0:\n",
    "                    eval_rsqa_cdd = {}\n",
    "                    eval_rsqa_cdd[cdd] = res.rsquared_adj\n",
    "                    best_fit_cdd = {}\n",
    "                    best_fit_cdd[cdd] = res\n",
    "                    #counter += 1\n",
    "                else:\n",
    "                    continue\n",
    "        best_fit_all.update(best_fit_cdd)\n",
    "        eval_rsqa_all.update(eval_rsqa_cdd)\n",
    "        \n",
    "        best_fit_hdd = {}\n",
    "        eval_rsqa_hdd = {}\n",
    "        counter = 0\n",
    "        for hdd in hdd_cols_checked:    \n",
    "            all_vars_hdd = all_vars + [hdd]\n",
    "            res = sm.OLS(designm['meter_value'], designm[all_vars_hdd], missing='drop').fit()\n",
    "            if save_candidate_results:\n",
    "                candidate_all_list.append({hdd: res})\n",
    "            \n",
    "            if counter == 0:\n",
    "                eval_rsqa_hdd[hdd] = res.rsquared_adj\n",
    "                best_fit_hdd[hdd] = res\n",
    "                counter += 1\n",
    "                continue\n",
    "            else:\n",
    "                params_dict = res.params.to_dict()\n",
    "                pvalues_dict = res.pvalues.to_dict()\n",
    "                h_slope = params_dict[hdd]\n",
    "                h_slope_p = pvalues_dict[hdd]\n",
    "                intercept = params_dict['const']\n",
    "                if list(eval_rsqa_hdd.values())[0] < res.rsquared_adj and h_slope > 0 and h_slope_p < .1 and intercept >= 0:\n",
    "                    eval_rsqa_hdd = {}\n",
    "                    eval_rsqa_hdd[hdd] = res.rsquared_adj\n",
    "                    best_fit_hdd = {}\n",
    "                    best_fit_hdd[hdd] = res\n",
    "                    #counter += 1\n",
    "                else:\n",
    "                    continue\n",
    "        best_fit_all.update(best_fit_hdd)\n",
    "        eval_rsqa_all.update(eval_rsqa_hdd)\n",
    "        \n",
    "        best_fit_cdd_hdd = {}\n",
    "        eval_rsqa_cdd_hdd = {}\n",
    "        counter = 0\n",
    "        for x in itertools.product(cdd_cols_checked, hdd_cols_checked):\n",
    "            if x[0][-2:] > x[1][-2:]:\n",
    "                cdd = x[0]\n",
    "                hdd = x[1]\n",
    "                all_vars_cdd_hdd = all_vars + [cdd, hdd]\n",
    "                res = sm.OLS(designm['meter_value'], designm[all_vars_cdd_hdd], missing='drop').fit()\n",
    "                if save_candidate_results:\n",
    "                    candidate_all_list.append({cdd + \"+\" + hdd: res})                \n",
    "                \n",
    "                if counter == 0:\n",
    "                    eval_rsqa_cdd_hdd[cdd + \"+\" + hdd] = res.rsquared_adj\n",
    "                    best_fit_cdd_hdd[cdd + \"+\" + hdd] = res\n",
    "                    counter += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    params_dict = res.params.to_dict()\n",
    "                    pvalues_dict = res.pvalues.to_dict()\n",
    "                    c_slope = params_dict[cdd]\n",
    "                    c_slope_p = pvalues_dict[cdd]\n",
    "                    h_slope = params_dict[hdd]\n",
    "                    h_slope_p = pvalues_dict[hdd]\n",
    "                    intercept = params_dict['const']\n",
    "                    if list(eval_rsqa_cdd_hdd.values())[0] < res.rsquared_adj and c_slope > 0 and c_slope_p < .1 and h_slope > 0 and h_slope_p < .1 and intercept >= 0:\n",
    "                        eval_rsqa_cdd_hdd = {}\n",
    "                        eval_rsqa_cdd_hdd[cdd + \"+\" + hdd] = res.rsquared_adj\n",
    "                        best_fit_cdd_hdd = {}\n",
    "                        best_fit_cdd_hdd[cdd + \"+\" + hdd] = res\n",
    "                        #counter += 1\n",
    "                    else:\n",
    "                        continue\n",
    "        best_fit_all.update(best_fit_cdd_hdd)\n",
    "        eval_rsqa_all.update(eval_rsqa_cdd_hdd)\n",
    "        \n",
    "        # Compare the best cdd, the best hdd, and the best cdd+hdd\n",
    "        \n",
    "        # See whether each of the best threes is validated\n",
    "        eval_best_fit_all = {}\n",
    "        for pnt, res in best_fit_all.items():\n",
    "            if '+' not in pnt:\n",
    "                if res.params.to_dict()[pnt] > 0 and res.pvalues.to_dict()[pnt] < .1 and res.params.to_dict()['const'] >= 0:\n",
    "                    eval_best_fit_all[pnt] = {\"status\":\"validated\"}\n",
    "                    eval_best_fit_all[pnt].update({\"model_res\": res})\n",
    "                else:\n",
    "                    eval_best_fit_all[pnt] = {\"status\":\"not validated\"}\n",
    "                    eval_best_fit_all[pnt].update({\"model_res\": res})\n",
    "            else:\n",
    "                c_pnt = pnt.split('+')[0]\n",
    "                h_pnt = pnt.split('+')[1]\n",
    "\n",
    "                if res.params.to_dict()[c_pnt] > 0 and res.pvalues.to_dict()[c_pnt] < .1 and res.params.to_dict()[h_pnt] > 0 and res.pvalues.to_dict()[h_pnt] < .1 and res.params.to_dict()['const'] >= 0:\n",
    "                    eval_best_fit_all[pnt] = {\"status\":\"validated\"}\n",
    "                    eval_best_fit_all[pnt].update({\"model_res\": res})\n",
    "                else:\n",
    "                    eval_best_fit_all[pnt] = {\"status\":\"not validated\"}\n",
    "                    eval_best_fit_all[pnt].update({\"model_res\": res})\n",
    "        \n",
    "        # Put the validation status into a list\n",
    "        status_list = []\n",
    "        for item in list(eval_best_fit_all.values()):\n",
    "            status_list.append(item['status'])\n",
    "        \n",
    "        # Get the best model based on the number of validated models\n",
    "        final_contest = {}\n",
    "        if 'not validated' not in status_list: # Do the original\n",
    "            best_fit_all = {max(eval_rsqa_all, key=eval_rsqa_all.get): best_fit_all[max(eval_rsqa_all, key=eval_rsqa_all.get)]}\n",
    "        elif status_list.count('not validated') == 3: # Do the original\n",
    "            best_fit_all = {max(eval_rsqa_all, key=eval_rsqa_all.get): best_fit_all[max(eval_rsqa_all, key=eval_rsqa_all.get)]}\n",
    "        elif status_list.count('not validated') == 2: # The only validated one is the best model\n",
    "            for k,v in eval_best_fit_all.items():\n",
    "                if v['status'] == 'validated':\n",
    "                    best_fit_all = {k: v['model_res']}\n",
    "                else:\n",
    "                    continue\n",
    "        else: # From the two validated models, the best model is the one that has the greater adjusted r-squared \n",
    "            eval_rsqa_besttwo = {}\n",
    "            for k,v in eval_best_fit_all.items():\n",
    "                if v['status'] == 'validated':\n",
    "                    final_contest.update({k: v['model_res']})\n",
    "                    eval_rsqa_besttwo.update({k: v['model_res'].rsquared_adj})\n",
    "                else:\n",
    "                    continue\n",
    "            best_fit_all = {max(eval_rsqa_besttwo, key=eval_rsqa_besttwo.get): final_contest[max(eval_rsqa_besttwo, key=eval_rsqa_besttwo.get)]}\n",
    "        \n",
    "        \n",
    "        # Compare with the intercept only model\n",
    "        if list(best_fit_all.values())[0].rsquared_adj < intercept_only_rsqa:\n",
    "            best_fit_all = {}\n",
    "            best_fit_all['intercept_only'] = res_intercept_only\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        balance_point_str = list(best_fit_all.keys())[0]\n",
    "        if balance_point_str == \"intercept_only\":\n",
    "            designm_droppedNA = designm[all_vars + ['meter_value']].dropna()\n",
    "            y_bar = designm_droppedNA['meter_value'].mean()\n",
    "            p = len(all_vars)\n",
    "            best_fit_all.update({'y_bar': y_bar})\n",
    "            best_fit_all.update({'n_predictors': p})\n",
    "        elif \"+\" not in balance_point_str:\n",
    "            designm_droppedNA = designm[all_vars + ['meter_value', balance_point_str]].dropna()\n",
    "            y_bar = designm_droppedNA['meter_value'].mean()\n",
    "            p = len(all_vars + [balance_point_str])\n",
    "            best_fit_all.update({'y_bar': y_bar})\n",
    "            best_fit_all.update({'n_predictors': p})\n",
    "        else:\n",
    "            designm_droppedNA = designm[all_vars + ['meter_value', balance_point_str.split('+')[0], balance_point_str.split('+')[1]]].dropna()\n",
    "            y_bar = designm_droppedNA['meter_value'].mean()\n",
    "            p = len(all_vars + [balance_point_str.split('+')[0], balance_point_str.split('+')[1]])\n",
    "            best_fit_all.update({'y_bar': y_bar})\n",
    "            best_fit_all.update({'n_predictors': p})\n",
    "        \n",
    "        if save_candidate_results:\n",
    "            return best_fit_all, candidate_all_list \n",
    "        else:\n",
    "            return best_fit_all\n",
    "\n",
    "def append_cvrmse(best_fit_dict):\n",
    "    y_bar = best_fit_dict['y_bar']\n",
    "    p = best_fit_dict['n_predictors']\n",
    "    res = list(best_fit_dict.values())[0]\n",
    "    n = res.nobs\n",
    "    ssr = res.ssr\n",
    "    cvrmse = (ssr/(n-p))**(.5) / y_bar\n",
    "    best_fit_dict.update({'cvrmse': cvrmse}) \n",
    "    return best_fit_dict\n",
    "        \n",
    "def extract_best_fit(best_fit_dict):\n",
    "    best_fit_usable = {}\n",
    "    res = list(best_fit_dict.values())[0]\n",
    "    params_dict = res.params.to_dict()\n",
    "    pvalues_dict = res.pvalues.to_dict()\n",
    "    balance_point_str = list(best_fit_dict.keys())[0]\n",
    "    \n",
    "    cvrmse = best_fit_dict['cvrmse']\n",
    "    y_bar = best_fit_dict['y_bar']\n",
    "    p = best_fit_dict['n_predictors']\n",
    "    \n",
    "    if balance_point_str == \"intercept_only\":\n",
    "        best_fit_usable['model'] = {'model_type': 'intercept_only', \n",
    "                                        'formula': 'meter_value ~ intercept',\n",
    "                                        'observed_length': res.nobs}\n",
    "        best_fit_usable['model_params'] = {'intercept': params_dict['const']\n",
    "                                          }\n",
    "        best_fit_usable['model_fit'] = {'r_squared': res.rsquared,\n",
    "                                        'r_squared_adj': res.rsquared_adj,\n",
    "                                        'bic': res.bic,\n",
    "                                        'ssr': res.ssr,\n",
    "                                        'cvrmse': cvrmse\n",
    "                                        }\n",
    "    \n",
    "    elif \"+\" not in balance_point_str:\n",
    "        if balance_point_str.startswith(\"cdd\"):\n",
    "            balance_point = int(balance_point_str.split('_')[1])\n",
    "            best_fit_usable['model'] = {'model_type': 'cdd_only', \n",
    "                                        'formula': 'meter_value ~ ' + balance_point_str,\n",
    "                                        'observed_length': res.nobs}\n",
    "            best_fit_usable['model_params'] = {'intercept': params_dict['const'],\n",
    "                                               'beta_cdd': params_dict[balance_point_str],\n",
    "                                               'beta_cdd_pvalue': pvalues_dict[balance_point_str],\n",
    "                                              'cooling_balance_point': balance_point}\n",
    "            best_fit_usable['model_fit'] = {'r_squared': res.rsquared,\n",
    "                                            'r_squared_adj': res.rsquared_adj,\n",
    "                                            'bic': res.bic,\n",
    "                                            'ssr': res.ssr,\n",
    "                                            'cvrmse': cvrmse\n",
    "                                            }\n",
    "        if balance_point_str.startswith(\"hdd\"):\n",
    "            balance_point = int(balance_point_str.split('_')[1])\n",
    "            best_fit_usable['model'] = {'model_type': 'hdd_only', \n",
    "                                        'formula': 'meter_value ~ ' + balance_point_str,\n",
    "                                        'observed_length': res.nobs}\n",
    "            best_fit_usable['model_params'] = {'intercept': params_dict['const'],\n",
    "                                               'beta_hdd': params_dict[balance_point_str],\n",
    "                                               'beta_hdd_pvalue': pvalues_dict[balance_point_str],\n",
    "                                              'heating_balance_point': balance_point}\n",
    "            best_fit_usable['model_fit'] = {'r_squared': res.rsquared,\n",
    "                                            'r_squared_adj': res.rsquared_adj,\n",
    "                                            'bic': res.bic,\n",
    "                                            'ssr': res.ssr,\n",
    "                                            'cvrmse': cvrmse\n",
    "                                            }\n",
    "    else:\n",
    "        c_balance_point = int(balance_point_str.split('+')[0].split('_')[1])\n",
    "        h_balance_point = int(balance_point_str.split('+')[1].split('_')[1])\n",
    "        best_fit_usable['model'] = {'model_type': 'cdd + hdd', \n",
    "                                    'formula': 'meter_value ~ ' + balance_point_str,\n",
    "                                    'observed_length': res.nobs}\n",
    "        best_fit_usable['model_params'] = {'intercept': params_dict['const'],\n",
    "                                           'beta_cdd': params_dict[balance_point_str.split('+')[0]],\n",
    "                                           'beta_cdd_pvalue': pvalues_dict[balance_point_str.split('+')[0]],\n",
    "                                           'beta_hdd': params_dict[balance_point_str.split('+')[1]],\n",
    "                                           'beta_hdd_pvalue': pvalues_dict[balance_point_str.split('+')[1]],\n",
    "                                          'cooling_balance_point': c_balance_point,\n",
    "                                          'heating_balance_point': h_balance_point}\n",
    "        best_fit_usable['model_fit'] = {'r_squared': res.rsquared,\n",
    "                                        'r_squared_adj': res.rsquared_adj,\n",
    "                                        'bic': res.bic,\n",
    "                                        'ssr': res.ssr,\n",
    "                                        'cvrmse': cvrmse\n",
    "                                        }\n",
    "    return best_fit_usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "Run the whole thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is needed when you analyze the whole dataset (large data)\n",
    "# The goal here is to further divide the 10,000-sized chunks into 50-sized subchunks\n",
    "\n",
    "# Currently commented out:\n",
    "# tenthou_l = range(0,10000)\n",
    "# tenthou_chunks = [tenthou_l[i:i + 50] for i in range(0, len(tenthou_l), 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks = list(listdir_nohidden(data_chunks_dir))\n",
    "chunks = sorted(chunks, key=lambda x: int(re.sub('\\D', '', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_chunk01.json', 'data_chunk02.json', 'data_chunk03.json']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the operating system, create an empty folder called 'res' under the current working directory\n",
    "# then, the following code will create a folder for each chunk under the 'res' folder\n",
    "# done already, so commented out\n",
    "\n",
    "# for chunk in chunks:\n",
    "#     res_chunk_dir = \"res\" + chunk[4:12]\n",
    "#     os.makedirs(os.path.join(cwd, \"res\", res_chunk_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you reopen a notebook:\n",
    "# Directly load the saved intermediate data:\n",
    "\n",
    "# a\n",
    "with open(os.path.join(cwd, 'intermediate_data', 'chunks_location_station.pickle'), 'rb') as fp:\n",
    "    chunks_location_station = pickle.load(fp)\n",
    "# b\n",
    "with open(os.path.join(cwd, 'intermediate_data', 'chunks_coord_metstation.pickle') as fp:\n",
    "    chunks_coord_metstation = pickle.load(fp)\n",
    "# c\n",
    "df_met_usable = pd.read_csv(os.path.join(cwd, 'intermediate_data', 'df_met_usable.csv'), encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\lulinghu\\\\Documents\\\\CLIR\\\\projects\\\\energy_inequality\\\\comed\\\\scripts\\\\code_for_sharing\\\\intermediate_data\\\\chunks_location_station.pickle'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(cwd, 'intermediate_data', 'chunks_location_station.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(cwd, 'intermediate_data', 'chunks_location_station.pickle'), 'rb') as fp:\n",
    "    chunks_location_station = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model on: data_chunk01.json\n",
      "Running model on: data_chunk02.json\n",
      "Running model on: data_chunk03.json\n"
     ]
    }
   ],
   "source": [
    "# if no subchunk is used:\n",
    "for chunk in chunks:\n",
    "    print(\"Running model on:\", chunk)\n",
    "    res_chunk_dir = \"res\" + chunk[4:12]\n",
    "    \n",
    "    with open(os.path.join(data_chunks_dir, chunk), 'r') as fp:\n",
    "        chunk_data = json.load(fp)\n",
    "        \n",
    "    id_res_l = {}\n",
    "    for k, v in chunk_data.items():\n",
    "        elec_l = []\n",
    "        date_l = []\n",
    "        for key, val in v.items():\n",
    "            if key.endswith('_elec'):\n",
    "                elec_l = elec_l + val\n",
    "            if key.endswith('date'):\n",
    "                date_l = date_l + val\n",
    "        \n",
    "        date_l = [datetime.datetime.strptime(str(x), \"%Y%m%d\").date().strftime(\"%m/%d/%Y\") for x in date_l]\n",
    "        \n",
    "        elec_l = elec_l + [np.nan]\n",
    "        date_l = date_l + ['01/01/2018']\n",
    "\n",
    "        datetime_series = pd.to_datetime(date_l)\n",
    "\n",
    "        meter_data = pd.DataFrame(\n",
    "                        {\"value\": elec_l},\n",
    "                        index=datetime_series\n",
    "                    )\n",
    "\n",
    "        meter_data = meter_data.tz_localize('UTC')\n",
    "\n",
    "        coord = v['location']\n",
    "        tempF = chunks_location_station[coord]['tempF']\n",
    "\n",
    "        design_matrix = eemeter.create_caltrack_daily_design_matrix(meter_data, tempF)\n",
    "        design_matrix = fix_rows(design_matrix)\n",
    "        design_matrix = create_day_vars(design_matrix, start_date, end_date)\n",
    "\n",
    "        # Remove time from the datetimeindex\n",
    "        # Purpose: Make it easier to append cols from met data in the next line\n",
    "        design_matrix.index = design_matrix.index.date\n",
    "\n",
    "        design_matrix = append_met_cols(design_matrix, df_met_usable, coord, chunks_coord_metstation)\n",
    "        design_matrix = sm.add_constant(design_matrix)\n",
    "\n",
    "        try:\n",
    "            fit_all_best = fit_caltrack_customized(design_matrix,\n",
    "                                                   save_candidate_results=False)\n",
    "        except Exception as e:\n",
    "            print(k, e)\n",
    "            continue\n",
    "\n",
    "        best_fit_usable_dict = extract_best_fit(append_cvrmse(fit_all_best))\n",
    "\n",
    "        id_res_l[k] = {'coord': coord}\n",
    "        id_res_l[k].update({'accttype': v['accttype']})\n",
    "\n",
    "        # Add the result from each ID to a big dictionary\n",
    "        id_res_l[k].update(best_fit_usable_dict)\n",
    "        \n",
    "    # Save result to file\n",
    "    res_fn = 'res' + '_' + 'zipn_customized_met_lou_' + chunk[5:12] +'.json'\n",
    "\n",
    "    with open(os.path.join(cwd, \"res\", res_chunk_dir, res_fn), 'w') as fp:\n",
    "        json.dump(id_res_l, fp)\n",
    "\n",
    "    # To save RAM:\n",
    "    del id_res_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if subchunks are used:\n",
    "# e.g., when the full sample is analyzed\n",
    "for chunk in chunks:\n",
    "    print(\"Running model on:\", chunk)\n",
    "    res_chunk_dir = \"res\" + chunk[4:12]\n",
    "    \n",
    "    with open(os.path.join(data_chunks_dir, chunk), 'r') as fp:\n",
    "        chunk_data = json.load(fp)\n",
    "    \n",
    "    for rg in tenthou_chunks:\n",
    "        start = rg[0]\n",
    "        end = rg[49]\n",
    "        id_res_l = {}\n",
    "        \n",
    "        for k, v in islice(chunk_data.items(), start, end+1, 1):\n",
    "            \n",
    "            elec_l = []\n",
    "            date_l = []\n",
    "            for key, val in v.items():\n",
    "                if key.endswith('_elec'):\n",
    "                    elec_l = elec_l + val\n",
    "                if key.endswith('date'):\n",
    "                    date_l = date_l + val\n",
    "                    #print(key + ':', len(val))\n",
    "            #print(len(energy_l))\n",
    "            #print(len(date_l))\n",
    "\n",
    "            date_l = [datetime.datetime.strptime(str(x), \"%Y%m%d\").date().strftime(\"%m/%d/%Y\") for x in date_l]\n",
    "\n",
    "            elec_l = elec_l + [np.nan]\n",
    "            date_l = date_l + ['01/01/2018']\n",
    "\n",
    "            datetime_series = pd.to_datetime(date_l)\n",
    "\n",
    "            meter_data = pd.DataFrame(\n",
    "                            {\"value\": elec_l},\n",
    "                            index=datetime_series\n",
    "                        )\n",
    "\n",
    "            meter_data = meter_data.tz_localize('UTC')\n",
    "\n",
    "            coord = v['location']\n",
    "            tempF = chunks_location_station[coord]['tempF']\n",
    "\n",
    "            design_matrix = eemeter.create_caltrack_daily_design_matrix(meter_data, tempF)\n",
    "            design_matrix = fix_rows(design_matrix)\n",
    "            design_matrix = create_day_vars(design_matrix, start_date, end_date)\n",
    "\n",
    "            # Remove time from the datetimeindex\n",
    "            # Purpose: Make it easier to append cols from met data in the next line\n",
    "            design_matrix.index = design_matrix.index.date\n",
    "\n",
    "            design_matrix = append_met_cols(design_matrix, df_met_usable, coord, chunks_coord_metstation)\n",
    "            design_matrix = sm.add_constant(design_matrix)\n",
    "\n",
    "            try:\n",
    "                fit_all_best = fit_caltrack_customized(design_matrix,\n",
    "                                                       save_candidate_results=False)\n",
    "            except Exception as e:\n",
    "                print(k, e)\n",
    "                continue\n",
    "\n",
    "            best_fit_usable_dict = extract_best_fit(fit_all_best)\n",
    "\n",
    "            id_res_l[k] = {'coord': coord}\n",
    "            id_res_l[k].update({'accttype': v['accttype']})\n",
    "\n",
    "            # Add the result from each ID to a big dictionary\n",
    "            id_res_l[k].update(best_fit_usable_dict)\n",
    "\n",
    "        # Save result to file\n",
    "        res_fn = 'res' + '_' + str(start) + '_' + str(end) + '_' + 'zipn_customized_met_lou_' + chunk[5:12] +'.json'\n",
    "\n",
    "        with open(os.path.join(cwd, \"res\", res_chunk_dir, res_fn), 'w') as fp:\n",
    "            json.dump(id_res_l, fp)\n",
    "\n",
    "        # To save RAM:\n",
    "        del id_res_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Read best-fit results saved in json and create a dataframe for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0\n",
    "Load results from json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = os.path.join(cwd,'res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_res = {}\n",
    "for j in list(listdir_nohidden(res_dir)):\n",
    "    for i in listdir_nohidden(res_dir + '/' + j):\n",
    "        with open(res_dir + '/' + j + '/' + i, 'r') as filehandle:\n",
    "            res = json.load(filehandle)\n",
    "        chunks_res.update(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_res) # looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_res_usable = {}\n",
    "for k,v in chunks_res.items():\n",
    "    chunks_res_usable[k] = {'coord': v['coord']} \n",
    "    chunks_res_usable[k].update({'accttype': v['accttype']})\n",
    "    \n",
    "    try:\n",
    "        chunks_res_usable[k].update({'intercept': v['model_params']['intercept']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        chunks_res_usable[k].update({'c_pnt': v['model_params']['cooling_balance_point']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        chunks_res_usable[k].update({'c_pnt': v['model_params']['cooling_balance_point']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        chunks_res_usable[k].update({'h_pnt': v['model_params']['heating_balance_point']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        chunks_res_usable[k].update({'c_slope': v['model_params']['beta_cdd']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        chunks_res_usable[k].update({'c_slope_p': v['model_params']['beta_cdd_pvalue']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        chunks_res_usable[k].update({'h_slope': v['model_params']['beta_hdd']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        chunks_res_usable[k].update({'h_slope_p': v['model_params']['beta_hdd_pvalue']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        chunks_res_usable[k].update({'rsqa': v['model_fit']['r_squared_adj']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        chunks_res_usable[k].update({'cvrmse': v['model_fit']['cvrmse']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        chunks_res_usable[k].update({'bic': v['model_fit']['bic']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        chunks_res_usable[k].update({'observed_len': v['model']['observed_length']})\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        chunks_res_usable[k].update({'model_type': v['model']['model_type']})\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame.from_dict(chunks_res_usable, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1\n",
    "Screen invalid results:\n",
    "The following cases should be excluded from further analysis\n",
    "* Any slope is negative OR\n",
    "* Any p value of slope >= 0.1 OR\n",
    "* rsqa is missing OR\n",
    "* rsqa is -inf OR\n",
    "* cvrmse is missing OR\n",
    "* cvrmse is inf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
